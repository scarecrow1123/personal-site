{
  
    
        "post0": {
            "title": "OMS CS6601 Artificial Intelligence Spring '20",
            "content": "CS 6601 Artificial Intelligence . Instructor: Thad Starner Course Page: Link . This is my second year in the OMS program and CS6601 proved to be way more serious than the previous ones that I had taken before. The course covered a larger number of topics than the previous ones, the projects and content being more difficult. The students definitely aged a bit at the end of the term and a student even claimed that he started having delusional Piazza/Canvas notifications about the grades in his dreams. On the other hand, finishing this course also was more satisfactory given the number of hours we spent each week. . About the course The course is a survey of AI algorithms/paradigms and closely follows Artificial Intelligence: A Modern Approach, by Stuart Russel and Peter Norvig book. It starts with discussing the technique involved in developing game AI called Adversarial Search which includes algorithms such as Minimax, Expectimax, Iterative Deepening &amp; Alpha-beta pruning. This topic gets a companion project where we had to implement these techniques to develop an agent that can play a board game called Isolation. Following this, a survey of various Search algorithms used for Planning such as Breadth First Search, Depth First Search, A*, etc. were introduced. The second project involved implementing these algorithms along with extending these techniques to cases such as bi-directional and tri-directional searches for effective planning. . Next were lessons on Constraint Satisfaction Problems(CSPs) and Simulated Annealing techniques. This was followed by lectures as well as a project on Bayes Networks. This introduced techniques such as d-separation, variable elimination, etc. for probabilistic inference. A very brief introduction to different Machine Learning algorithms was given and a subsequent project to implement a few variants of Decision Trees. Another project related to Machine Learning was to implement Image Segmentation using Gaussian Mixture Models. We were also introduced to Hidden Markov Models and had to implement a simple sign language recognition model as a project. At the end, there was a brief discussion about Propsitional &amp; First-order Logic, followed by an introduction to Markov Decision Processes. . There were six projects in total from which one project with the least grade was dropped for grades. The projects summed up to 60% of the total grade. Most of the students found the first two projects very hard and one had to spend 20-30 hours on the projects in general. Two take home, open book exams fill up the rest of the grade distribution. The exams were long 50+ page booklets and involved coding up and solving problems from the above topics. . Lecture Content The lectures tend to overlook at places without going deeper into the material. Apart from the Russel &amp; Norvig book, I found lectures from MIT AI - Patrick Winston, Stanford AI - Liang &amp; Sadigh, UC Berkeley AI - Peter Abeel and other resources very helpful to fill in the gaps. Few videos from the channel mathematicalmonk also helped for certain topics. . Conclusion This class was a hard, but satisfactory course. As with any other course, starting the projects as early as possible is the key. Starting early in this course is even more crucial given the insanely long number of hours each project requires, especially if one works full time. .",
            "url": "https://scarecrow1123.github.io/personal-site/gatech-omscs/2020/05/10/ai.html",
            "relUrl": "/gatech-omscs/2020/05/10/ai.html",
            "date": " • May 10, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Audio pre-processing for Machine Learning: Getting things right",
            "content": "Audio pre-processing for Machine Learning: Getting things right . For any machine learning experiment, careful handling of input data in terms of cleaning, encoding/decoding, featurizing are paramount. When it comes to applying machine learning for audio, it gets even trickier when compared with text/image, since dealing with audio involves many tiny details that can be overlooked. Any sort of inconsistency in the pre-processing pipeline could be a potential disaster in terms of the final accuracy of the overall system. We’ll look into a few basic things that need to be set right when writing an audio pre-processing pipeline. . If you are not familiar with how audio input is fed to a machine learning model, I highly recommend reading these two articles first: How to do Speech Recognition with Deep Learning, Speech Processing for Machine Learning - Filter banks, etc. . Fixing on a data format . First step to get the pipeline right is to fix on a specific data format that the system would require. This would ensure a consistent interface that the dataset reader can rely upon. The usual practice is to use WAV which is a lossless format(FLAC is also another popular choice). Since WAV is an uncompressed format, it tends to be better when compared to lossy formats such as MP3, etc. . Do not vary the sample rate . WAV stores audio signals as a series of numbers also called the PCM (Pulse Code Modulation) data. PCM is a way to convert analog audio to digital data. So essentially if you are loading an audio file into a numpy array, it is the underlying PCM data that is loaded. Each number in the sequence is called a sample, that represents the amplitude of the signal at an approximate point in time. It is called a sample since the PCM method approximates the amplitude value by sampling the original audio signal for a fixed number of times every second. The number of samples taken for every second is the sampling rate of the signal. This is an important factor that needs to be uniform in the audio pipeline. If this varies in different parts of a system, things can get miserable! Many machine learning systems for audio applications such as speech recognition, wake-word detection, etc. can work well with 16k Hz audio(16000 samples for every second of the original audio). So for example, a numpy array for a 5 second audio with 16k Hz sample rate would have the shape (80000,) ( 5 * 16000 = 80000). . Popular audio libraries such as PySoundFile, audiofile, librosa, etc. in Python provide operations for loading audio to numpy array and return the sample rate of the signal. The libraries use the header information in WAV files to figure out the sample rate. . # Using soundfile to load audio and know its sample rate import soundfile as sf audio, sample_rate = sf.read(&quot;sample.wav&quot;) . Bit depth . This is a crucial property that needs to be handled correctly, especially in places where the data is loaded to arrays/tensors. Bit depth represents the number of bits required to represent each sample in the PCM audio data. In practice, 16-bit signed integers can be used to store training data. During training, these 16-bit data can be loaded to 32-bit float tensors/arrays and can be fed to neural nets. Things can go wrong here say when a 24-bit audio file is loaded into a 16-bit array. Let’s take Python stdlib’s wave module for example, which returns a byte array from an audio file: . import wave w = wave.open(&quot;sample_16b.wav&quot;, &quot;rb&quot;) n_samples = w.getnframes() audio_data = w.readnframes(n_samples) . The byte array is converted into a np array using np.frombuffer and specifying the appropriate type of the data stored, 16-bit int in this case. Things will go wrong when it is loaded into a wrong container say np.int8 . # correct type audio_array = np.frombuffer(audio_data, dtype=np.int16) # incorrect audio_array = np.frombuffer(audio_data, dtype=np.int8) . Hence deciding on a standard bit depth that the system will always look for, will help eliminate overflows because of incorrect typecasting. . Byte order . It is also recommended to not to take the byte order for granted when reading/writing audio data. Even though the underlying codec may take into account the system’s byte order, for the paranoid ones, it is better to get fixed on one standard order, say little endian. . Channels . Number of channels can depend on the actual application for which the pre-processing step is done. For speech recognition let’s say, an input to a neural net is typically a single channel. In case of a stereo input, each channel can form distinct inputs to the neural net. Or the channels could be merged together to form a mono audio. However, this is an application specific choice. . A standard way to load/convert input audio . To make sure nothing goes wrong in your audio pre-processing pipeline, it would be the safest to assume none of your inputs is in the right format and always go for a standard format conversion routine. Below would be a set of useful ffmpeg options using ffmpeg-python to standardize the incoming input: . import ffmpeg import numpy as np stream = ffmpeg.input(&quot;sample.wav&quot;) # set the output sample rate is 16000 stream = ffmpeg.filter(stream, &quot;aresample&quot;, osr=16000) # set num channels = 1, bit depth to 16-bit int(s16), byte order to little endian(le) stream = ffmpeg.output(stream, &quot;pipe:&quot;, ac=1, acodec=&quot;pcm_s16le&quot;, format=&quot;s16le&quot;) out, err = ffmpeg.run(stream, quiet=True) # load it with proper data type (int16) audio_array = np.frombuffer(out, dtype=np.int16) . Note that audio_array is raw PCM data and cannot be directly written into a WAV file. It is safe to use the IO mechanisms that the audio libraries provide to write the raw data into a WAV file. This will make sure appropriate headers are in place in the WAV file. . import soundfile as sf sf.write(&quot;sample_out.wav&quot;, audio_array, samplerate=16000, subtype=&quot;PCM_16&quot;, endian=&quot;LITTLE&quot;) . The raw array data however is the starting point for further pre-processing which depend on the downstream experiment/application. They can be converted to signal processing features such as spectrogram, MFCC, etc. which are supported by libraries such as librosa, torchaudio, etc. .",
            "url": "https://scarecrow1123.github.io/personal-site/python/deep-learning/software/2020/01/01/audio.html",
            "relUrl": "/python/deep-learning/software/2020/01/01/audio.html",
            "date": " • Jan 1, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "OMS CS7646 Machine Learning for Trading- Fall '19",
            "content": "CS 7646 Machine Learning For trading . Instructor(s): David Byrd / David Joyner / Tucker Balch Course Page: Link . This was my third OMS course after Robotics - AI Techniques and KBAI. I’ve only been taking one course per term and this marks the completion of my one year into OMS, starting from Spring ‘19. . About the Course . This is a gentle introduction to few fundamental concepts of numpy/pandas, machine learning and trading. The only prerequisite that this course assumes is some familiarity in programming with Python. Hence this could be an apt starter course for someone from a non CS background too. . The first part of the course deals with introducing numpy, pandas, i.e., dealing with multi dimensional data in general. Historical stock data are provided in CSV files. Basic manipulation and plotting these data with matplotlib is also introduced. . Second part introduces concepts of trading, technical analysis and a some insights of how hedge funds function. Technical analysis chapter deals with details on how simple heuristics called technical indicators are used by traders to understand various characteristics of a stock. These indicators form the feature set with which machine learning algorithms are trained to do automated trading. . Third part discusses basic machine learning algorithms such as linear regression, decision trees and also Q-Learning which is a reinforcement learning technique. . Projects . There are totally 8 projects involved in the course work. Few of the major projects are: . Decision Trees: In this project, we built a regression model with decision trees from scratch. A classic decision tree and a random forest with boosting is built to predict stock returns. . | Market Simulation: The objective here is to understand how a stock market works by writing a toy market simulator. It keeps track of orders/holdings and compute the final statistics. The code written for this also would find place in the subsequent projects. . | Manual Strategy: Here, the intention is to implement few of the technical indicators of choice. Along with these, we are to write a rule based system to incorporate the chosen indicators and do trading on the simulator that is already written. . | Q-Learner: The objective here is to implement a generic Q-Learner, but for robot navigation primarily to get familiarized with the algorithm. . | The final project is to use either the Decision Tree or the Q-Learner and replace the rule-based trading with the automated one and report findings. . | . Class and Grading . This term, the on-site and online classes were run together for the first time as an experiment. Only the exams and lecture delivery varied. The class was pretty much run by the head TA for the entire term. Grades were not curved with two exams amounting to 25% and projects to 73% of the total grades. There were minor points for an extra credit project and participation. . Conclusion . The technical analysis part was helpful for me personally. Since I had prior experience with Python and machine learning to an extent, the material wasn’t difficult to follow. I wish the course content had a bit more depth at times and the material felt like undergrad level in a few places. .",
            "url": "https://scarecrow1123.github.io/personal-site/gatech-omscs/2019/12/31/ml4t.html",
            "relUrl": "/gatech-omscs/2019/12/31/ml4t.html",
            "date": " • Dec 31, 2019"
        }
        
    
  
    
        ,"post3": {
            "title": "How to handle multi process logging in Python?",
            "content": "Python’s logging module provides a list of super useful handlers to handle/redirect log messages to required target destinations. For instance FileHandler sends the messages to a file, DatagramHandler sends to UDP ports, etc. In a multi process setup however, using FileHandler to redirect logs to the same file from different processes would only corrupt the log file. Explicitly acquiring locks to the same file is a bad thing to do. . An out-of-the-box way is to use this package called multiprocess-logging. This pip package implements a custom log handler. The handler receives all the messages, puts them in an internal queue and emits by dequeuing messages from the queue. . Aggregating the log messages to a queue is the only way to handle this scenario. But there is a graceful alternative way to achieve this. Python’s logging module provides built in mechanisms to handle queue based logging with QueueHandler and QueueListener classes. . Similar to how FileHandler provides a way to do file based logging, QueueHandler helps to log to a queue object. This queue can be from the queue module or multiprocessing.Queue. . import logging from logging.handlers import QueueHandler from queue import SimpleQueue logging.warning(&quot;Test this warning in the console&quot;) q = SimpleQueue() queue_handler = QueueHandler(q) root_logger = logging.getLogger() # remove the StreamHandler instance that is set by default root_logger.handlers.pop() # add queue handler # this will stop the log messages from printing in the console root_logger.addHandler(queue_handler) logging.warning(&quot;Test this warning in the queue&quot;) print(q.get()) . This prints: . WARNING:root:Test this warning in the console &lt;LogRecord: root, 30, test.py, 14, &quot;Test this warning in the queue&quot;&gt; . Note the second printed line is only because of the print function. The second logging.warning function has consumed the message and put it in the q object. . QueueListener is used along with the QueueHandler to collect messages from the queue and push them to various other targets such as a file, UDP port, etc. . ... file_handler = FileHandler(&quot;out.log&quot;) listener = QueueListener(q, file_handler) listener.start() ... . The listener looks for messages in q and pushes them to file_handler which writes to out.log. Let’s use the same flow in a multi process setup. . Assume a master process that spawns 4 children and all log messages are to be written to out.log file. Any logger message from the children will be written to a multiprocessing.Queue. The children need to initialize a QueueHandler and attach it to their root logger as seen above to achieve this. The same queue is listened at the master process’s QueueListener. . Here’s how the master process’s logging is setup: . def setup_primary_logging(): log_queue = mp.Queue(-1) # Handlers for stream/file logging output_file_log_handler = logging.FileHandler(filename=&quot;out.log&quot;) formatter = logging.Formatter(&#39;%(asctime)s - %(levelname)s - %(message)s&#39;) output_file_log_handler.setFormatter(formatter) output_file_log_handler.setLevel(logging.INFO) # This listener listens to the `log_queue` and pushes the messages to the list of # handlers specified. listener = QueueListener(log_queue, output_file_log_handler, error_file_log_handler, respect_handler_level=True) listener.start() return log_queue . And the worker’s logging setup: . def setup_worker_logging(log_queue): queue_handler = QueueHandler(log_queue) queue_handler.setLevel(logging.INFO) root_logger = logging.getLogger() root_logger.addHandler(queue_handler) # Default logger level is WARNING, hence the change. Otherwise, any worker logs # are not going to get bubbled up to the parent&#39;s logger handlers from where the # actual logs are written to the output root_logger.setLevel(logging.INFO) . Note that log_queue object has to be sent from the master to all the children. So the master process looks something like this: . def worker_fn(log_queue): setup_worker_logging(log_queue) def master_fn(): log_queue = setup_primary_logging() mp.spawn(worker_fn, args=(log_queue,), n_procs=4) . Any messages from the worker will be logged to out.log file with the above setup. A complete working example can be seen in this gist. .",
            "url": "https://scarecrow1123.github.io/personal-site/python/software/2019/09/26/multi-log.html",
            "relUrl": "/python/software/2019/09/26/multi-log.html",
            "date": " • Sep 26, 2019"
        }
        
    
  
    
        ,"post4": {
            "title": "OMS CS7638 Robotics - AI Techniques - Summer '19",
            "content": "CS 7638 Robotics - AI Techniques . Instructor(s): Jay Summet / Sebastian Thrun Course Page: Link . This happened to be my second OMS course following KBAI. Previously known as AI for Robotics(AI4R popularly), this isn’t supposed to be originally designed for a summer(short) term. This is typically a 16-week long course which got shrunk to a 11-week one and we were duly cautioned by the professor about this at the beginning of the term. . Course Videos . The course follows the content from Sebastian Thrun’s Udacity videos. The video lectures broadly covers the following: . Localization Histogram Filters | Kalman Filters | Particle Filters | . | Search / Path Planning | PID Control | SLAM (Simultaneous Localization and Planning) | . The video content are fairly simple to follow as Prof Thrun makes it sound easy in a lot of places. However, at times, the details were skimmed over and not a lot of background motivation were given for certain parts of the lecture material. Each module is followed by a problem set which comes with solution videos. Solving them and submitting them on canvas fetches 28% of the total grade of the entire class. These were not hard to solve and are not time consuming too. . Projects . The projects are the crux of this class as there are no tests/exams in this class. All projects are auto-graded and the solutions had to be adopted from the lecture content. For most of the projects, code from the problem sets served as the boilerplate and were highly helpful to set something up working quickly. The following projects were part of the Summer class: . Asteroids - Kalman Filter | Mars Glider - Particle Filter | Rocket PID - PID Control | Warehouse - Motion Planning and Search | Ice Rover - SLAM | . Asteroids and Mars Glider projects were being introduced for the first time as a part of CS7638 and we were the lab rats of some sorts. There was too much FOMO especially during the Mars Glider project weeks, as there were bugs in the testing system and also the test cases were being changed often as this was the first time they introduced the project. We also had to spend a lot of time in tuning parameters to make these systems work well. All of these projects deal with tackling noise in the measurements and motion of robots, obviously one had to get the parameters right to make the systems stable. People spent more time in tuning the params than they did for coding up the whole thing. So those who started late(like me in a couple of projects) suffered from lower scores. Starting early on the projects especially Mars Glider and Warehouse is advisable for anyone who do not want to miss on an A. . There were also constraints that were introduced in the projects which were not part of the lectures nor the problem sets. Examples being the Q-matrix in Kalman Filters, “fuzzing” in Mars Glider (which automatically made my code work). Typically none of us knew what they were about until the the teaching staff explained the entire thing and helped the class through. . Class and Grading . Prof Summet and the TAs did an excellent job in answering the student questions during the entire period of the class. Since there were a lot of curve balls thrown at us during the projects especially, their insights were very useful in coding up the projects and clarifying the doubts from the problem sets. The Slack group was also highly active with healthy discussions and suggestions. . Grading is absolute and the project scores are the only ones taken into account for the final grades apart from the problem set grades. Problem sets can be taken for granted as they are there only to check if one has finished watching the lectures. . Material . I personally did not follow Prof Thrun’s Probabilistic Robotics book which was the suggested text. This particular e-book Kalman and Bayesian Filters in Python is an excellent to resource for anyone to understand and get an intuition about these topics with straightforward implementation details. . Conclusion . Comparing to my previous KBAI class, there are no reports to be written and having code only projects are a huge plus. Optimizing for the auto-grader score games up the work being done and could turn to be fun, but only if you start early on for the projects! .",
            "url": "https://scarecrow1123.github.io/personal-site/gatech-omscs/2019/07/31/rait.html",
            "relUrl": "/gatech-omscs/2019/07/31/rait.html",
            "date": " • Jul 31, 2019"
        }
        
    
  
    
        ,"post5": {
            "title": "Exploring Jsonnet",
            "content": "I found Jsonnet through AllenNLP. Hence a few words on that first. We use AllenNLP for writing our Deep Learning experiments in our team. It is primarily built for doing NLP research on top of PyTorch. However, the abstractions in the library are well designed and easily extensible that it can actually be used for building any fairly straightforward neural network experiments. Perhaps I would write a separate post on how to adopt AllenNLP for non-NLP experiments, but here is an example of how it has been used for Computer Vision. . Jsonnet . Jsonnet is a DSL for creating data templates and comes in handy to generate JSON based configuration data. It comes with a standard library std that includes features like list comprehension, string manipulation, etc. It is primarily meant for generating configuration files. std has a bunch of manifestation utilities that can be used to convert the template to generate targets in .ini, .yaml. For a robust templating language with more complex needs however, I’d suggest to use the awesome StringTemplate. . AllenNLP uses Jsonnet for writing experiment configurations. In other words, the dependencies for running an AllenNLP experiment are specified in a .jsonnet file and the objects are constructed using built in factories. Below is a section from a configuration that defines a simple feedforward MNIST classifier network: . Sample Configuration . mnist_feedforward.jsonnet . { // ..... &quot;model&quot;: { &quot;mnist_encoder&quot;: { &quot;num_layers&quot;: 2, &quot;activations&quot;: [&quot;relu&quot;, &quot;relu&quot;], &quot;input_dim&quot;: 784, &quot;hidden_dims&quot;: 512, &quot;dropout&quot;: 0.25 }, &quot;projection_layer&quot;: { &quot;num_layers&quot;: 1, &quot;activations&quot;: &quot;relu&quot;, &quot;input_dim&quot;: 512, &quot;hidden_dims&quot;: 64, &quot;dropout&quot;: 0.25 }, &quot;final_layer&quot;: { &quot;num_layers&quot;: 1, &quot;activations&quot;: &quot;linear&quot;, &quot;input_dim&quot;: 64, &quot;hidden_dims&quot;: 10 } } // ..... } . Variables . One minor quibble with the above config: when running multiple experiments, the most obvious thing one would do is to change those numbers in every layer. Say to increase the output dimension of the mnist_encoder, input_dim of projection_layer needs to be adjusted too. For a more complex architecture, there is a good possibility that this would lead to a chain of changes to be done manually. . Obvious thing to do now is to use variables. In the below example, notice how the same variable is used for configuring both the output and input sizes of mnist_encoder and projection_layer layers respectively. . mnist_feedforward.jsonnet . local INPUT_SIZE = 784; local INPUT_ENCODER_OUTPUT_SIZE = 512; local PROJECTION_SIZE = 64; local FINAL_OUTPUT_SIZE = 2; local DROPOUT = 0.25; { // ..... &quot;model&quot;: { &quot;mnist_encoder&quot;: { &quot;num_layers&quot;: 2, &quot;activations&quot;: [&quot;relu&quot;, &quot;relu&quot;], &quot;input_dim&quot;: INPUT_SIZE, &quot;hidden_dims&quot;: INPUT_ENCODER_OUTPUT_SIZE, &quot;dropout&quot;: DROPOUT }, &quot;projection_layer&quot;: { &quot;num_layers&quot;: 1, &quot;activations&quot;: &quot;relu&quot;, &quot;input_dim&quot;: INPUT_ENCODER_OUTPUT_SIZE, &quot;hidden_dims&quot;: PROJECTION_SIZE, &quot;dropout&quot;: 0.25 }, &quot;final_layer&quot;: { &quot;num_layers&quot;: 1, &quot;activations&quot;: &quot;linear&quot;, &quot;input_dim&quot;: PROJECTION_SIZE, &quot;hidden_dims&quot;: FINAL_OUTPUT_SIZE } } // ..... } . Objects . The next natural step of the experiment is to try different types of layers. In the above example mnist_encoder is a feedforward block. It could be a convolution based encoder as below: . mnist_conv.jsonnet . local ConvSpec = { &quot;num_layers&quot;: 2, &quot;input_dim&quot;: 784, &quot;kernels&quot;: [[3, 3], [3, 3]], &quot;stride&quot;: [1, 1], &quot;activations&quot;: [&quot;relu&quot;, &quot;relu&quot;], &quot;output_channels&quot;: [32, 64] }; { // .... &quot;model&quot;: { &quot;mnist_encoder&quot;: { &quot;type&quot;: &quot;conv2d&quot; &quot;num_layers&quot;: ConvSpec.num_layers, &quot;input_dim&quot;: ConvSpec.input_dim, &quot;output_channels&quot;: ConvSpec.output_channels, &quot;kernels&quot;: ConvSpec.kernels, &quot;stride&quot;: ConvSpec.stride, &quot;activations&quot;: ConvSpec.activations }, // .... }, // ..... } . Here ConvSpec is a jsonnet object with a bunch of member attributes that defines how the convolution block is used in the classifier. Jsonnet also supports inheritance of objects as shown here. . Functions . The subsequent layers such as projection_layer and final_layer are going to be present in the conv example too and the projection_layer needs an input size to be defined. This will be output size of the conv layer and hardcoding this number is going to cause the same set of problems that we saw above in the first example. Let’s define a simple function that computes the output sizes of each layer in a conv block. . local conv_output(dim, kernel, stride, padding, dilation) = std.floor(((dim + 2 * padding - dilation * (kernel - 1) -1) / stride) + 1); local compute_conv_output_sizes(conv_, input_size, axis, curr_idx=0, sizes=[]) = if curr_idx &gt;= conv_.num_layers then sizes else if std.length(sizes) == 0 then compute_conv_output_sizes(conv_, input_size, axis, curr_idx+1, [conv_output(input_size, conv_.kernels[curr_idx][axis], conv_.stride[curr_idx][axis], conv_.padding[curr_idx][axis], conv_.dilation[curr_idx][axis])]) else compute_conv_output_sizes(conv_, input_size, axis, curr_idx+1, sizes + [conv_output(sizes[std.length(sizes)-1], conv_.kernels[curr_idx][axis], conv_.stride[curr_idx][axis], conv_.padding[curr_idx][axis], conv_.dilation[curr_idx][axis])]); . compute_conv_output_sizes is a tail recursive function calculates the output size of each conv layer based on the defined kernel size, stride, padding and dilation. . Let’s incorporate this definition in the mnist_encoder example: . mnist_conv.jsonnet . local ConvSpec = { &quot;num_layers&quot;: 2, &quot;input_dim&quot;: 784, &quot;kernels&quot;: [[3, 3], [3, 3]], &quot;stride&quot;: [1, 1], &quot;activations&quot;: [&quot;relu&quot;, &quot;relu&quot;], &quot;output_channels&quot;: [32, 64], &quot;output_sizes&quot;: compute_conv_output_sizes(self, self.input_dim, 0), &quot;output_size&quot;: self.output_sizes[-1] }; { // .... &quot;model&quot;: { &quot;mnist_encoder&quot;: { &quot;type&quot;: &quot;conv2d&quot; &quot;num_layers&quot;: ConvSpec.num_layers, &quot;input_dim&quot;: ConvSpec.input_dim, &quot;output_channels&quot;: ConvSpec.output_channels, &quot;kernels&quot;: ConvSpec.kernels, &quot;stride&quot;: ConvSpec.stride, &quot;activations&quot;: ConvSpec.activations }, // .... }, // ..... } . Now a layer that follows mnist_encoder can make use of ConvSpec.output_size to configure its input sizes. . Imports . So we have defined two variants of encoders here for a classifier. Except for the encoder all the other parts of the model configuration and training configuration are going to be the same. Let’s put the base scaffolding that defines the classifier in a .libsonnet file. This will serve as an importable lib that two different experiments can use. . lib-mnist.libsonnet . { MNIST(encoder, projection_size, final_output_size, dropout, num_projection_layers): { // ..... &quot;model&quot;: { &quot;mnist_encoder&quot;: encoder, &quot;projection_layer&quot;: { &quot;num_layers&quot;: num_projection_layers, &quot;activations&quot;: &quot;relu&quot;, &quot;input_dim&quot;: if std.objectHas(encoder, &quot;type&quot;) then encoder.output_size else encoder.hidden_dims, &quot;hidden_dims&quot;: projection_size, &quot;dropout&quot;: dropout }, &quot;final_layer&quot;: { &quot;num_layers&quot;: 1, &quot;activations&quot;: &quot;linear&quot;, &quot;input_dim&quot;: projection_size, &quot;hidden_dims&quot;: final_output_size } } // ..... } } . Notice above how the MNIST classifier has been parameterized which can be used from different variants of the architecture. Now, let’s redefine the feedforward variant to do the import. . mnist_feedforward.jsonnet . local lib = import &quot;lib-mnist.libsonnet&quot;; local INPUT_SIZE = 784; local INPUT_ENCODER_OUTPUT_SIZE = 512; local PROJECTION_SIZE = 64; local FINAL_OUTPUT_SIZE = 2; local DROPOUT = 0.25; local NUM_PROJECTION_LAYERS = 1; local ENCODER = { &quot;spec&quot;: { &quot;num_layers&quot;: 2, &quot;activations&quot;: [&quot;relu&quot;, &quot;relu&quot;], &quot;input_dim&quot;: INPUT_SIZE, &quot;hidden_dims&quot;: INPUT_ENCODER_OUTPUT_SIZE, &quot;dropout&quot;: DROPOUT } }; lib.MNIST(ENCODER.spec, PROJECTION_SIZE, FINAL_OUTPUT_SIZE, DROPOUT, NUM_PROJECTION_LAYERS); . mnist_conv.jsonnet . local lib = import &quot;lib-mnist.libsonnet&quot;; local INPUT_SIZE = 784; local PROJECTION_SIZE = 64; local FINAL_OUTPUT_SIZE = 2; local DROPOUT = 0.25; local NUM_PROJECTION_LAYERS = 1; local ConvSpec = { &quot;num_layers&quot;: 2, &quot;input_dim&quot;: INPUT_SIZE, &quot;kernels&quot;: [[3, 3], [3, 3]], &quot;stride&quot;: [1, 1], &quot;activations&quot;: [&quot;relu&quot;, &quot;relu&quot;], &quot;output_channels&quot;: [32, 64], &quot;output_sizes&quot;: compute_conv_output_sizes(self, self.input_dim, 0), &quot;output_size&quot;: self.output_sizes[-1] }; local ConvEncoder = { &quot;spec&quot;: { &quot;type&quot;: &quot;conv2d&quot; &quot;num_layers&quot;: ConvSpec.num_layers, &quot;input_dim&quot;: ConvSpec.input_dim, &quot;output_channels&quot;: ConvSpec.output_channels, &quot;kernels&quot;: ConvSpec.kernels, &quot;stride&quot;: ConvSpec.stride, &quot;activations&quot;: ConvSpec.activations } }; lib.MNIST(ConvEncoder.spec, PROJECTION_SIZE, FINAL_OUTPUT_SIZE, DROPOUT, NUM_PROJECTION_LAYERS) . Now this kind of a setup would allow to easily bring in rapid prototyping and experimentation. Just replace the encoder with RNN or Self Attention based layers and pass it to lib.MNIST. . Jsonnet brings in a lot of flexibility to defining configurations as we have seen above. There are a lot more interesting things that could be done with respect to configuring neural net experiments with Jsonnet. Imagine writing a grid search procedure in Jsonnet that would generate all possible configurations for the different hyperparameter combinations. That wouldn’t be too difficult I guess. A lot of interesting example experiment configurations can be found in AllenNLP’s source here. .",
            "url": "https://scarecrow1123.github.io/personal-site/deep-learning/software/2019/07/06/jsonnet.html",
            "relUrl": "/deep-learning/software/2019/07/06/jsonnet.html",
            "date": " • Jul 6, 2019"
        }
        
    
  
    
        ,"post6": {
            "title": "OMS CS7637 - Knowledge-Based AI (KBAI) - Spring '19",
            "content": "CS7637 Knowledge-Based AI - Cognitive Systems (KBAI) - Spring ‘19 . Instructor: Prof David Joyner Course Page: Link . I applied for OMSCS in 2018 and joined the Spring ‘19 batch. I intend to do Machine Learning(ML) Specialization. KBAI is in no way related to anything that is machine learning and it does not fall under the core/elective courses that is required for the specialization. To satisfy the specialization requirements, I will be taking the following seven courses that are available throughout this program: . CS 6515 GA | CS7641 ML | CS6476 CV | CS7642 RL | CS7646 ML4T | CSE6242 DVA | CSE6250 BD4H | . Apart from the above courses few other courses that may be relevant for someone who is interested in ML/AI could be CS6601 Artifical Intelligence, CS7638 AI for Robotics. KBAI can be considered as a distant precursor to a typical AI course, but definitely not a prerequisite if one is planning to take any of the above courses. . About KBAI . KBAI is a gentle introduction to concepts and problems that are involved when designing an AI algorithm. The syllabus surveys a wide variety of traditional AI paradigms and concepts that closely follows the Patrick Winston “Artificial Intelligence” book. The course content also includes some of the work done personally by the instructors Ashok Goel, David Joyner and people from their lab. . Course work . The goal of this class is to develop an AI agent to solve a variant of an IQ test called Raven’s Progressive Matrices (RPM). The class is divided into three 5-week periods. Each of these include submitting a 10-page assignment, project work and an exam. The project work includes writing code to solve a given set of RPM problems and writing a detailed report. So at the end of the course everyone would have submitted 3 sets of assignments, projects and exams. . RPM Project . There are 4 sets of RPM problems given with increasing difficulty(see attached image for a sample problem). These problems could be solved in two different ways. For the first two projects, we would be given a verbal description of these patterns which can be used as input to solve the problem. However for the final project, only images can be directly used as inputs. Hence it would make sense to start the first project itself by using image inputs rather than starting with verbal and changing the agent implementation at a later stage to use visual inputs. These two papers[1], [2] helped a lot to implement the solution successfully. . Getting started early on the projects is also helpful. The grading rubric for the project includes how spaced out the submissions are for a project. . About the class . The class was very well organized. I should say that Prof Joyner’s way of organizing a class should be a blueprint for designing and conducting classes online. The TAs were also helpful throughout in Piazza and the forums were literally buzzing throughout(perhaps because posting something would fetch a student some participation grades too ;)). But overall the class was very engaging and set the tone for me on how OMS is going to be. The class also used Peerfeedback to receive and give peer feedback about project and assignment submissions. Even though the feedbacks I received were mostly generic and sometimes rhetorical, getting to read others’ submissions was definitely helpful in a lot of ways to me personally. . One quibble I had with the content was that the things I(or most of the class) implemented for the projects were for a large part looked disconnected from the lecture content. Even though on hindsight a bunch of things from an implementation could be mapped to a few concepts in the lectures, there was nothing that enforced on using them. Also, the lecture content looked repetitive and hand-wavy a bit at times. There are not many computational techniques/models that one could learn from this class. . There are also a couple of optional participation projects completing which would fetch some participation points. Both of them had an NLP problem to solve. These projects are testers for what may be given as full time projects in one of the later versions of KBAI. Another interesting fact about KBAI is from one of the discussions that happened in the forums. It looks like in the Fall version of KBAI, the TAs wouldn’t know if a submission is from the online class or the onsite class that happens at GT. And I heard that there is no much performance difference between the two classes. . Conclusion . I opted this class because this was my first semester into the program. A lot of reviews in OMSCentral also suggested this class for someone who is starting with OMS. I also intended to get an introduction to the traditional AI topics and this class did satisfy a part of my expectations. However, if you are expecting to learn computational techniques, this class is weak on that front. This is an easy class for a graduate level program and one can expect to work around 10-15 hours a week during submission times. .",
            "url": "https://scarecrow1123.github.io/personal-site/gatech-omscs/2019/05/05/kbai.html",
            "relUrl": "/gatech-omscs/2019/05/05/kbai.html",
            "date": " • May 5, 2019"
        }
        
    
  
    
        ,"post7": {
            "title": "About Georgia Tech OMSCS",
            "content": "Online Master of Science Computer Science (OMSCS) . Georgia Tech(GT) introduced their Online Master’s program way back in 2014 IIRC. I remember reading one of the early Hacker News(HN) threads about the program when it was introduced. MOOCs were at rise during that period of time. But doing an entire Master’s program online was definitely new to me then and realized how cool the offering is when I discussed about it with my uncle who actually sent me the HN thread. On hindsight, now I understand how far ahead GT was in offering this course. For a perspective, lots of other universities such as Arizona, UIUC, etc. have started similar courses only very recently, while few thousands have already graduated from OMSCS in the last few years. . Back to 2018 . I decided to enrol myself into the program after realizing the importance of going through a high standard curriculum of a Master’s program. I was trying to learn the basics of Deep Learning myself from a bunch of great graduate level resources. My routine was to mostly watch lectures, read related material, take notes and apply them at work. Even though this exposed me to a lot of graduate level content, I still wasn’t committed much to the material because I wasn’t going through a formal curriculum that would include assignments, grades and stuff. . Choosing OMSCS . I started looking into the online graduate level programs. The primary factors that I looked into these programs were: . Credibility | Quality of course content | Affordability | Community | . Credibility &amp; Course content . GT has been a pioneer with more than 2000 alumni from OMS and I’ve heard good things about the program through my friends who are already enrolled to the program. So I had no second thoughts on the credibility of this program. People have even got into some top PhD programs after OMSCS. . The course listing is actually not bad. I agree that there may not be courses like Deep Learning which I was primarily interested on. However there’s a lot to learn from the current offering. The Machine Learning related courses do look solid. All of the courses are produced exclusively for the online program and many courses are siblings of the onsite offering but just optimized for an online experience. . Affordability . OMSCS scores big when it comes to affordability. The tuition fee for a single 3 credit course is ~ $500 and term fee is ~ $300. In comparison, the UIUC program may cost up to 3 times that of OMSCS. . Community . Doing a remote degree does have rough edges early on as a fresher into the program. Things can go wrong any time with respect to the classroom experience as well as administrative stuff such as registration, payment, etc. Troubleshooting is not easy even with the help of thorough documentation. OMSCS has got multiple friendly online communities at Reddit, Slack, Facebook, MeWe. The now defunct G+ community was the best of all though. These are all well moderated student run communities and people are extremely friendly and help you throughout. There is also OMSCentral volunteer project where students write their reviews and comments about the courses they’ve taken. This is super helpful in getting to validating and choosing courses that may suit you personally. . Applying . I applied in August 2018 for Spring ‘19 enrollment. The process involved submitting a TOEFL score along with a SoP, resume, background essay and 3 LoRs. OMSCS has a rolling admission process. It means that one can submit an application round the year. But there are cutoff dates for respective semesters. One can get enrolled into either Spring/Fall of a year. The academic calendar of OMSCS is the same as that of the onsite courses. I received the institute decision by October first week. I’ve heard the acceptance rate is pretty high at more than 60%, but I’m not sure. The reddit community actively maintains an admission thread which may shed more information on why one may get rejected if that is the case. . Getting into the program . Students will receive a detailed orientation document before the course registration starts. Things may get very anxious before the start of the registration. The G+ community was very helpful in clearing the doubts early and this set of Orientation videos by Prof Joyner turned out to be very helpful to get to know about how the courses would be run. . Program info . OMSCS requires one to complete 10 three credit courses to receive a degree. The actual degree offered does not differ from the one that is given for an onsite MS program. However, the transcripts would mention the campus as Online if that would matter. The schedule is a standard one following a three semester per year schedule which is the same as the onsite one. One can take upto 2 courses in Spring &amp; Fall and 1 in Summer. If one wishes to take an extra course, they need to get permission from the advising team. OMSCS also offers different specializations which may help to channelize one’s choice of courses throughout the program. Every specialization has core and elective courses requirement that needs to be satisfied. . Conclusion . I’m enjoying OMSCS so far and would definitely recommend to anyone who may be interested. I just finished my first semester(Spring ‘19) with CS7637 and will be writing about it in another post soon. People have written a lot about this program online and also given finer details about the courses. They have all been helpful for me in getting to know about this program. I intend to do the same thing by logging my OMS experience. .",
            "url": "https://scarecrow1123.github.io/personal-site/gatech-omscs/2019/05/04/about-gatech.html",
            "relUrl": "/gatech-omscs/2019/05/04/about-gatech.html",
            "date": " • May 4, 2019"
        }
        
    
  
    
        ,"post8": {
            "title": "Python Control Flow: EAFP Vs LBYL",
            "content": "EAFP - Easier to Ask for Forgiveness than Permission LBYL - Look Before You Leap . These are two different ways to do control flow. LBYL style pertains to writing if/else blocks to make decisions. According to [1], in this not so standard Pythonic way of doing control flow, exceptional cases get the emphasis by the way the conditions are expressed. A common example as below: . if &quot;key&quot; in dict_: value += dict_[&quot;key&quot;] . As [1] suggests, in simpler words, the above piece of code conveys the special case in an emphasized way rather than showing us what is normal. Using try/except blocks(EAFP), we write what is normal and handle exceptions that may rise out of it. This becomes easier to convey the more natural cases as follows: . try: value += dict_[&quot;key&quot;] except KeyError: pass . [2] adds another important case where EAFP helps to avoid race conditions. In a multi-threaded environment, in the above if/else block, assume a thread has passed the if condition. Before the next statement gets executed in the current thread, another thread may inadvertently remove the key from dict_ which would cause an exception in the original thread. However, in the try/except case, this cannot happen. Another way to solve this problem is to use standard locking mechanisms. . [1] - Idiomatic Python: EAFP versus LBYL . [2] - Chapter 15. Context Managers and else Blocks, Fluent Python, Luciano Ramalho .",
            "url": "https://scarecrow1123.github.io/personal-site/python/2019/04/22/python.html",
            "relUrl": "/python/2019/04/22/python.html",
            "date": " • Apr 22, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am Ananda Seelan and this is my blog (or rather just a list of interesting things that I would like to note down for myself or a few others who might find it interesting). .",
          "url": "https://scarecrow1123.github.io/personal-site/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://scarecrow1123.github.io/personal-site/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}