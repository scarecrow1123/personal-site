{
  
    
        "post0": {
            "title": "fastcore: An Underrated Python Library",
            "content": ". Background . I recently embarked on a journey to sharpen my python skills: I wanted to learn advanced patterns, idioms, and techniques. I started with reading books on advanced Python, however, the information didn&#39;t seem to stick without having somewhere to apply it. I also wanted the ability to ask questions from an expert while I was learning -- which is an arrangement that is hard to find! That&#39;s when it occurred to me: What if I could find an open source project that has fairly advanced python code and write documentation and tests? I made a bet that if I did this it would force me to learn everything very deeply, and the maintainers would be appreciative of my work and be willing to answer my questions. . And that&#39;s exactly what I did over the past month! I&#39;m pleased to report that it has been the most efficient learning experience I&#39;ve ever experienced. I&#39;ve discovered that writing documentation forced me to deeply understand not just what the code does but also why the code works the way it does, and to explore edge cases while writing tests. Most importantly, I was able to ask questions when I was stuck, and maintainers were willing to devote extra time knowing that their mentorship was in service of making their code more accessible! It turns out the library I choose, fastcore is some of the most fascinating Python I have ever encountered as its purpose and goals are fairly unique. . For the uninitiated, fastcore is a library on top of which many fast.ai projects are built on. Most importantly, fastcore extends the python programming language and strives to eliminate boilerplate and add useful functionality for common tasks. In this blog post, I&#39;m going to highlight some of my favorite tools that fastcore provides, rather than sharing what I learned about python. My goal is to pique your interest in this library, and hopefully motivate you to check out the documentation after you are done to learn more! . Why fastcore is interesting . Get exposed to ideas from other languages without leaving python: I’ve always heard that it is beneficial to learn other languages in order to become a better programmer. From a pragmatic point of view, I’ve found it difficult to learn other languages because I could never use them at work. Fastcore extends python to include patterns found in languages as diverse as Julia, Ruby and Haskell. Now that I understand these tools I am motivated to learn other languages. | You get a new set of pragmatic tools: fastcore includes utilities that will allow you to write more concise expressive code, and perhaps solve new problems. | Learn more about the Python programming language: Because fastcore extends the python programming language, many advanced concepts are exposed during the process. For the motivated, this is a great way to see how many of the internals of python work. | A whirlwind tour through fastcore . Here are some things you can do with fastcore that immediately caught my attention. . . Making **kwargs transparent . Whenever I see a function that has the argument **kwargs, I cringe a little. This is because it means the API is obfuscated and I have to read the source code to figure out what valid parameters might be. Consider the below example: . def baz(a, b=2, c =3, d=4): return a + b + c def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, **kwargs)&gt; . Without reading the source code, it might be hard for me to know that foo also accepts and additional parameters b and d. We can fix this with delegates: . def baz(a, b=2, c =3, d=4): return a + b + c @delegates(baz) # this decorator will pass down keyword arguments from baz def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4)&gt; . You can customize the behavior of this decorator. For example, you can have your cake and eat it too by passing down your arguments and also keeping **kwargs: . @delegates(baz, keep=True) def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4, **kwargs)&gt; . You can also exclude arguments. For example, we exclude argument d from delegation: . def basefoo(a, b=2, c =3, d=4): pass @delegates(basefoo, but= [&#39;d&#39;]) # exclude `d` def foo(c, a, **kwargs): pass inspect.signature(foo) . &lt;Signature (c, a, b=2)&gt; . You can also delegate between classes: . class BaseFoo: def __init__(self, e, c=2): pass @delegates()# since no argument was passsed here we delegate to the superclass class Foo(BaseFoo): def __init__(self, a, b=1, **kwargs): super().__init__(**kwargs) inspect.signature(Foo) . &lt;Signature (a, b=1, c=2)&gt; . For more information, read the docs on delegates. . . Avoid boilerplate when setting instance attributes . Have you ever wondered if it was possible to avoid the boilerplate involved with setting attributes in __init__? . class Test: def __init__(self, a, b ,c): self.a, self.b, self.c = a, b, c . Ouch! That was painful. Look at all the repeated variable names. Do I really have to repeat myself like this when defining a class? Not Anymore! Checkout store_attr: . class Test: def __init__(self, a, b, c): store_attr() t = Test(5,4,3) assert t.b == 4 . You can also exclude certain attributes: . class Test: def __init__(self, a, b, c): store_attr(but=[&#39;c&#39;]) t = Test(5,4,3) assert t.b == 4 assert not hasattr(t, &#39;c&#39;) . There are many more ways of customizing and using store_attr than I highlighted here. Check out the docs for more detail. . . Avoiding subclassing boilerplate . One thing I hate about python is the __super__().__init__() boilerplate associated with subclassing. For example: . class ParentClass: def __init__(self): self.some_attr = &#39;hello&#39; class ChildClass(ParentClass): def __init__(self): super().__init__() cc = ChildClass() assert cc.some_attr == &#39;hello&#39; # only accessible b/c you used super . We can avoid this boilerplate by using the metaclass PrePostInitMeta. We define a new class called NewParent that is a wrapper around the ParentClass: . class NewParent(ParentClass, metaclass=PrePostInitMeta): def __pre_init__(self, *args, **kwargs): super().__init__() class ChildClass(NewParent): def __init__(self):pass sc = ChildClass() assert sc.some_attr == &#39;hello&#39; . . Type Dispatch . Type dispatch, or Multiple dispatch, allows you to change the way a function behaves based upon the input types it receives. This is a prominent feature in some programming languages like Julia. For example, this is a conceptual example of how multiple dispatch works in Julia, returning different values depending on the input types of x and y: . collide_with(x::Asteroid, y::Asteroid) = ... # deal with asteroid hitting asteroid collide_with(x::Asteroid, y::Spaceship) = ... # deal with asteroid hitting spaceship collide_with(x::Spaceship, y::Asteroid) = ... # deal with spaceship hitting asteroid collide_with(x::Spaceship, y::Spaceship) = ... # deal with spaceship hitting spaceship . Type dispatch can be especially useful in data science, where you might allow different input types (i.e. Numpy arrays and Pandas dataframes) to a function that processes data. Type dispatch allows you to have a common API for functions that do similar tasks. . Unfortunately, Python does not support this out-of-the box. Fortunately, there is the @typedispatch decorator to the rescue. This decorator relies upon type hints in order to route inputs the correct version of the function: . @typedispatch def f(x:str, y:str): return f&#39;{x}{y}&#39; @typedispatch def f(x:np.ndarray): return x.sum() @typedispatch def f(x:int, y:int): return x+y . Below is a demonstration of type dispatch at work for the function f: . f(&#39;Hello &#39;, &#39;World!&#39;) . &#39;Hello World!&#39; . f(2,3) . 5 . f(np.array([5,5,5,5])) . 20 . There are limitations of this feature, as well as other ways of using this functionality that you can read about here. In the process of learning about typed dispatch, I also found a python library called multipledispatch made by Mathhew Rocklin (the creator of Dask). . After using this feature, I am now motivated to learn languages like Julia to discover what other paradigms I might be missing. . . A better version of functools.partial . functools.partial is a great utility that creates functions from other functions that lets you set default values. Lets take this function for example that filters a list to only contain values &gt;= val: . test_input = [1,2,3,4,5,6] def f(arr, val): &quot;Filter a list to remove any values that are less than val.&quot; return [x for x in arr if x &gt;= val] f(test_input, 3) . [3, 4, 5, 6] . You can create a new function out of this function using partial that sets the default value to 5: . filter5 = partial(f, val=5) filter5(test_input) . [5, 6] . One problem with partial is that it removes the original docstring and replaces it with a generic docstring: . filter5.__doc__ . &#39;partial(func, *args, **keywords) - new function with partial application n of the given arguments and keywords. n&#39; . fastcore.utils.partialler fixes this, and makes sure the docstring is retained such that the new API is transparent: . filter5 = partialler(f, val=5) filter5.__doc__ . &#39;Filter a list to remove any values that are less than val.&#39; . . Composition of functions . A technique that is pervasive in functional programming languages is function composition, whereby you chain a bunch of functions together to achieve some kind of result. This is especially useful when applying various data transformations. Consider a toy example where I have three functions: (1) Removes elements of a list less than 5 (from the prior section) (2) adds 2 to each number (3) sums all the numbers: . def add(arr, val): return [x + val for x in arr] def arrsum(arr): return sum(arr) # See the previous section on partialler add2 = partialler(add, val=2) transform = compose(filter5, add2, arrsum) transform([1,2,3,4,5,6]) . 15 . But why is this useful? You might me thinking, I can accomplish the same thing with: . arrsum(add2(filter5([1,2,3,4,5,6]))) . You are not wrong! However, composition gives you a convenient interface in case you want to do something like the following: . def fit(x, transforms:list): &quot;fit a model after performing transformations&quot; x = compose(*transforms)(x) y = [np.mean(x)] * len(x) # its a dumb model. Don&#39;t judge me return y # filters out elements &lt; 5, adds 2, then predicts the mean fit(x=[1,2,3,4,5,6], transforms=[filter5, add2]) . [7.5, 7.5] . For more information about compose, read the docs. . . A more useful __repr__ . In python, __repr__ helps you get information about an object for logging and debugging. Below is what you get by default when you define a new class. (Note: we are using store_attr, which was discussed earlier). . class Test: def __init__(self, a, b=2, c=3): store_attr() # `store_attr` was discussed previously Test(1) . &lt;__main__.Test at 0x7fe0ab662790&gt; . We can use basic_repr to quickly give us a more sensible default: . class Test: def __init__(self, a, b=2, c=3): store_attr() __repr__ = basic_repr(&#39;a,b,c&#39;) Test(2) . Test(a=2, b=2, c=3) . . Monkey Patching With A Decorator . It can be convenient to monkey patch with a decorator, which is especially helpful when you want to patch an external library you are importing. We can use the decorator @patch from fastcore.foundation along with type hints like so: . class MyClass(int): pass @patch def func(self:MyClass, a): return self+a mc = MyClass(3) . Now, MyClass has an additional method named func: . mc.func(10) . 13 . Still not convinced? I&#39;ll show you another example of this kind of patching in the next section. . . A better pathlib.Path . When you see these extensions to pathlib.path you won&#39;t ever use vanilla pathlib again! A number of additional methods have been added to pathlib, such as: . Path.readlines: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.readlines() | Path.read: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.read() | Path.save: saves file as pickle | Path.load: loads pickle file | Path.ls: shows the contents of the path as a list. | etc. | . Read more about this here. Here is a demonstration of ls: . from pathlib import Path p = Path(&#39;../_notebooks&#39;) p.ls() # you don&#39;t get this with vanilla Pathlib.Path!! . (#21) [Path(&#39;../_notebooks/gpt2_simple_mask.jpg&#39;),Path(&#39;../_notebooks/bert_mac_small.jpg&#39;),Path(&#39;../_notebooks/causal_with_prefix.jpg&#39;),Path(&#39;../_notebooks/.DS_Store&#39;),Path(&#39;../_notebooks/2020-03-07-How_to_Create_an_Automatic_Code_Comment_Generator_using_Deep_Learning.ipynb&#39;),Path(&#39;../_notebooks/2020-09-01-fastcore.ipynb&#39;),Path(&#39;../_notebooks/2020-03-07-Syntax-Highlighting.ipynb&#39;),Path(&#39;../_notebooks/2020-03-06-bart.ipynb&#39;),Path(&#39;../_notebooks/README.md&#39;),Path(&#39;../_notebooks/2020-05-01-TrainDonkeyCar.ipynb&#39;)...] . Wait! What&#39;s going on here? We just imported pathlib.Path - why are we getting this new functionality? Thats because we imported the fastcore.foundation module, which patches this module via the @patch decorator discussed earlier. Just to drive the point home on why the @patch decorator is useful, I&#39;ll go ahead and add another method to Path right now: . @patch def fun(self:Path): return &quot;This is fun!&quot; p.fun() . &#39;This is fun!&#39; . That is magical, right? I know! That&#39;s why I&#39;m writing about it! . . An Even More Concise Way To Create Lambdas . Self, with an uppercase S, is an even more concise way to create lambdas that are calling methods on an object. For example, let&#39;s create a lambda for taking the sum of a Numpy array: . arr=np.array([5,4,3,2,1]) f = lambda a: a.sum() assert f(arr) == 15 . You can use Self in the same way: . f = Self.sum() assert f(arr) == 15 . Let&#39;s create a lambda that does a groupby and max of a Pandas dataframe: . import pandas as pd df=pd.DataFrame({&#39;Some Column&#39;: [&#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, ], &#39;Another Column&#39;: [5, 7, 50, 70]}) f = Self.groupby(&#39;Some Column&#39;).mean() f(df) . Another Column . Some Column . a 6 | . b 60 | . Read more about Self in the docs). . . Notebook Functions . These are simple but handy, and allow you to know whether or not code is executing in a Jupyter Notebook, Colab, or an Ipython Shell: . in_notebook(), in_colab(), in_ipython() . (True, False, True) . This is useful if you are displaying certain types of visualizations, progress bars or animations in your code that you may want to modify or toggle depending on the environment. . . A Drop-In Replacement For List . You might be pretty happy with Python&#39;s list. This is one of those situations that you don&#39;t know you needed a better list until someone showed one to you. Enter L, a list like object with many extra goodies. . The best way I can describe L is to pretend that list and numpy had a pretty baby: . define a list (check out the nice __repr__ that shows the length of the list!) . L(1,2,3) . (#3) [1,2,3] . Shuffle a list: . p = L.range(20).shuffle() p . (#20) [2,0,18,6,15,17,14,8,12,1...] . Index into a list: . p[2,4,6] . (#3) [18,15,14] . L has sensible defaults, for example appending an element to a list: . 1 + L(2,3,4) . (#4) [1,2,3,4] . There is much more L has to offer. Read the docs to learn more. . But Wait ... There&#39;s More! . There are more things I would like to show you about fastcore, but there is no way they would reasonably fit into a blog post. Here is a list of some of my favorite things that I didn&#39;t demo in this blog post: . Utilities . The Utilites section contain many shortcuts to perform common tasks or provide an additional interface to what standard python provides. . mk_class: quickly add a bunch of attributes to a class | wrap_class: add new methods to a class with a simple decorator | groupby: similar to Scala&#39;s groupby | merge: merge dicts | fasttuple: a tuple on steroids | Infinite Lists: useful for padding and testing | chunked: for batching and organizing stuff | . Multiprocessing . The Multiprocessing section extends python&#39;s multiprocessing library by offering features like: . progress bars | ability to pause to mitigate race conditions with external services | processing things in batches on each worker, ex: if you have a vectorized operation to perform in chunks | . Functional Programming . The functional programming section is my favorite part of this library. . maps: a map that also composes functions | mapped: A more robust map | using_attr: compose a function that operates on an attribute | . Transforms . Transforms is a collection of utilities for creating data transformations and associated pipelines. These transformation utilities build upon many of the building blocks discussed in this blog post. . Further Reading . It should be noted that you should read the main page of the docs first, followed by the section on tests to fully understand the documentation. . The fastcore documentation site. | The fastcore GitHub repo. | Blog post on delegation. | . Shameless plug: fastpages . This blog post was written entirely in a Jupyter Notebook, which GitHub automatically converted into to a blog post! Sound interesting? Check out fastpages. .",
            "url": "https://scarecrow1123.github.io/fastcore/",
            "relUrl": "/fastcore/",
            "date": " • Sep 1, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "OMS CS6601 Artificial Intelligence Spring '20",
            "content": "CS 6601 Artificial Intelligence . Instructor: Thad Starner Course Page: Link . This is my second year in the OMS program and CS6601 proved to be way more serious than the previous ones that I had taken before. The course covered a larger number of topics than the previous ones, the projects and content being more difficult. The students definitely aged a bit at the end of the term and a student even claimed that he started having delusional Piazza/Canvas notifications about the grades in his dreams. On the other hand, finishing this course also was more satisfactory given the number of hours we spent each week. . About the course The course is a survey of AI algorithms/paradigms and closely follows Artificial Intelligence: A Modern Approach, by Stuart Russel and Peter Norvig book. It starts with discussing the technique involved in developing game AI called Adversarial Search which includes algorithms such as Minimax, Expectimax, Iterative Deepening &amp; Alpha-beta pruning. This topic gets a companion project where we had to implement these techniques to develop an agent that can play a board game called Isolation. Following this, a survey of various Search algorithms used for Planning such as Breadth First Search, Depth First Search, A*, etc. were introduced. The second project involved implementing these algorithms along with extending these techniques to cases such as bi-directional and tri-directional searches for effective planning. . Next were lessons on Constraint Satisfaction Problems(CSPs) and Simulated Annealing techniques. This was followed by lectures as well as a project on Bayes Networks. This introduced techniques such as d-separation, variable elimination, etc. for probabilistic inference. A very brief introduction to different Machine Learning algorithms was given and a subsequent project to implement a few variants of Decision Trees. Another project related to Machine Learning was to implement Image Segmentation using Gaussian Mixture Models. We were also introduced to Hidden Markov Models and had to implement a simple sign language recognition model as a project. At the end, there was a brief discussion about Propsitional &amp; First-order Logic, followed by an introduction to Markov Decision Processes. . There were six projects in total from which one project with the least grade was dropped for grades. The projects summed up to 60% of the total grade. Most of the students found the first two projects very hard and one had to spend 20-30 hours on the projects in general. Two take home, open book exams fill up the rest of the grade distribution. The exams were long 50+ page booklets and involved coding up and solving problems from the above topics. . Lecture Content The lectures tend to overlook at places without going deeper into the material. Apart from the Russel &amp; Norvig book, I found lectures from MIT AI - Patrick Winston, Stanford AI - Liang &amp; Sadigh, UC Berkeley AI - Peter Abeel and other resources very helpful to fill in the gaps. Few videos from the channel mathematicalmonk also helped for certain topics. . Conclusion This class was a hard, but satisfactory course. As with any other course, starting the projects as early as possible is the key. Starting early in this course is even more crucial given the insanely long number of hours each project requires, especially if one works full time. .",
            "url": "https://scarecrow1123.github.io/gatech-omscs/2020/05/10/ai.html",
            "relUrl": "/gatech-omscs/2020/05/10/ai.html",
            "date": " • May 10, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://scarecrow1123.github.io/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Audio pre-processing for Machine Learning: Getting things right",
            "content": "Audio pre-processing for Machine Learning: Getting things right . For any machine learning experiment, careful handling of input data in terms of cleaning, encoding/decoding, featurizing are paramount. When it comes to applying machine learning for audio, it gets even trickier when compared with text/image, since dealing with audio involves many tiny details that can be overlooked. Any sort of inconsistency in the pre-processing pipeline could be a potential disaster in terms of the final accuracy of the overall system. We’ll look into a few basic things that need to be set right when writing an audio pre-processing pipeline. . If you are not familiar with how audio input is fed to a machine learning model, I highly recommend reading these two articles first: How to do Speech Recognition with Deep Learning, Speech Processing for Machine Learning - Filter banks, etc. . Fixing on a data format . First step to get the pipeline right is to fix on a specific data format that the system would require. This would ensure a consistent interface that the dataset reader can rely upon. The usual practice is to use WAV which is a lossless format(FLAC is also another popular choice). Since WAV is an uncompressed format, it tends to be better when compared to lossy formats such as MP3, etc. . Do not vary the sample rate . WAV stores audio signals as a series of numbers also called the PCM (Pulse Code Modulation) data. PCM is a way to convert analog audio to digital data. So essentially if you are loading an audio file into a numpy array, it is the underlying PCM data that is loaded. Each number in the sequence is called a sample, that represents the amplitude of the signal at an approximate point in time. It is called a sample since the PCM method approximates the amplitude value by sampling the original audio signal for a fixed number of times every second. The number of samples taken for every second is the sampling rate of the signal. This is an important factor that needs to be uniform in the audio pipeline. If this varies in different parts of a system, things can get miserable! Many machine learning systems for audio applications such as speech recognition, wake-word detection, etc. can work well with 16k Hz audio(16000 samples for every second of the original audio). So for example, a numpy array for a 5 second audio with 16k Hz sample rate would have the shape (80000,) ( 5 * 16000 = 80000). . Popular audio libraries such as PySoundFile, audiofile, librosa, etc. in Python provide operations for loading audio to numpy array and return the sample rate of the signal. The libraries use the header information in WAV files to figure out the sample rate. . # Using soundfile to load audio and know its sample rate import soundfile as sf audio, sample_rate = sf.read(&quot;sample.wav&quot;) . Bit depth . This is a crucial property that needs to be handled correctly, especially in places where the data is loaded to arrays/tensors. Bit depth represents the number of bits required to represent each sample in the PCM audio data. In practice, 16-bit signed integers can be used to store training data. During training, these 16-bit data can be loaded to 32-bit float tensors/arrays and can be fed to neural nets. Things can go wrong here say when a 24-bit audio file is loaded into a 16-bit array. Let’s take Python stdlib’s wave module for example, which returns a byte array from an audio file: . import wave w = wave.open(&quot;sample_16b.wav&quot;, &quot;rb&quot;) n_samples = w.getnframes() audio_data = w.readnframes(n_samples) . The byte array is converted into a np array using np.frombuffer and specifying the appropriate type of the data stored, 16-bit int in this case. Things will go wrong when it is loaded into a wrong container say np.int8 . # correct type audio_array = np.frombuffer(audio_data, dtype=np.int16) # incorrect audio_array = np.frombuffer(audio_data, dtype=np.int8) . Hence deciding on a standard bit depth that the system will always look for, will help eliminate overflows because of incorrect typecasting. . Byte order . It is also recommended to not to take the byte order for granted when reading/writing audio data. Even though the underlying codec may take into account the system’s byte order, for the paranoid ones, it is better to get fixed on one standard order, say little endian. . Channels . Number of channels can depend on the actual application for which the pre-processing step is done. For speech recognition let’s say, an input to a neural net is typically a single channel. In case of a stereo input, each channel can form distinct inputs to the neural net. Or the channels could be merged together to form a mono audio. However, this is an application specific choice. . A standard way to load/convert input audio . To make sure nothing goes wrong in your audio pre-processing pipeline, it would be the safest to assume none of your inputs is in the right format and always go for a standard format conversion routine. Below would be a set of useful ffmpeg options using ffmpeg-python to standardize the incoming input: . import ffmpeg import numpy as np stream = ffmpeg.input(&quot;sample.wav&quot;) # set the output sample rate is 16000 stream = ffmpeg.filter(stream, &quot;aresample&quot;, osr=16000) # set num channels = 1, bit depth to 16-bit int(s16), byte order to little endian(le) stream = ffmpeg.output(stream, &quot;pipe:&quot;, ac=1, acodec=&quot;pcm_s16le&quot;, format=&quot;s16le&quot;) out, err = ffmpeg.run(stream, quiet=True) # load it with proper data type (int16) audio_array = np.frombuffer(out, dtype=np.int16) . Note that audio_array is raw PCM data and cannot be directly written into a WAV file. It is safe to use the IO mechanisms that the audio libraries provide to write the raw data into a WAV file. This will make sure appropriate headers are in place in the WAV file. . import soundfile as sf sf.write(&quot;sample_out.wav&quot;, audio_array, samplerate=16000, subtype=&quot;PCM_16&quot;, endian=&quot;LITTLE&quot;) . The raw array data however is the starting point for further pre-processing which depend on the downstream experiment/application. They can be converted to signal processing features such as spectrogram, MFCC, etc. which are supported by libraries such as librosa, torchaudio, etc. .",
            "url": "https://scarecrow1123.github.io/python/deep-learning/software/2020/01/01/audio.html",
            "relUrl": "/python/deep-learning/software/2020/01/01/audio.html",
            "date": " • Jan 1, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "OMS CS7646 Machine Learning for Trading- Fall '19",
            "content": "CS 7646 Machine Learning For trading . Instructor(s): David Byrd / David Joyner / Tucker Balch Course Page: Link . This was my third OMS course after Robotics - AI Techniques and KBAI. I’ve only been taking one course per term and this marks the completion of my one year into OMS, starting from Spring ‘19. . About the Course . This is a gentle introduction to few fundamental concepts of numpy/pandas, machine learning and trading. The only prerequisite that this course assumes is some familiarity in programming with Python. Hence this could be an apt starter course for someone from a non CS background too. . The first part of the course deals with introducing numpy, pandas, i.e., dealing with multi dimensional data in general. Historical stock data are provided in CSV files. Basic manipulation and plotting these data with matplotlib is also introduced. . Second part introduces concepts of trading, technical analysis and a some insights of how hedge funds function. Technical analysis chapter deals with details on how simple heuristics called technical indicators are used by traders to understand various characteristics of a stock. These indicators form the feature set with which machine learning algorithms are trained to do automated trading. . Third part discusses basic machine learning algorithms such as linear regression, decision trees and also Q-Learning which is a reinforcement learning technique. . Projects . There are totally 8 projects involved in the course work. Few of the major projects are: . Decision Trees: In this project, we built a regression model with decision trees from scratch. A classic decision tree and a random forest with boosting is built to predict stock returns. . | Market Simulation: The objective here is to understand how a stock market works by writing a toy market simulator. It keeps track of orders/holdings and compute the final statistics. The code written for this also would find place in the subsequent projects. . | Manual Strategy: Here, the intention is to implement few of the technical indicators of choice. Along with these, we are to write a rule based system to incorporate the chosen indicators and do trading on the simulator that is already written. . | Q-Learner: The objective here is to implement a generic Q-Learner, but for robot navigation primarily to get familiarized with the algorithm. . | The final project is to use either the Decision Tree or the Q-Learner and replace the rule-based trading with the automated one and report findings. . | . Class and Grading . This term, the on-site and online classes were run together for the first time as an experiment. Only the exams and lecture delivery varied. The class was pretty much run by the head TA for the entire term. Grades were not curved with two exams amounting to 25% and projects to 73% of the total grades. There were minor points for an extra credit project and participation. . Conclusion . The technical analysis part was helpful for me personally. Since I had prior experience with Python and machine learning to an extent, the material wasn’t difficult to follow. I wish the course content had a bit more depth at times and the material felt like undergrad level in a few places. .",
            "url": "https://scarecrow1123.github.io/gatech-omscs/2019/12/31/ml4t.html",
            "relUrl": "/gatech-omscs/2019/12/31/ml4t.html",
            "date": " • Dec 31, 2019"
        }
        
    
  
    
        ,"post5": {
            "title": "How to handle multi process logging in Python?",
            "content": "Python’s logging module provides a list of super useful handlers to handle/redirect log messages to required target destinations. For instance FileHandler sends the messages to a file, DatagramHandler sends to UDP ports, etc. In a multi process setup however, using FileHandler to redirect logs to the same file from different processes would only corrupt the log file. Explicitly acquiring locks to the same file is a bad thing to do. . An out-of-the-box way is to use this package called multiprocess-logging. This pip package implements a custom log handler. The handler receives all the messages, puts them in an internal queue and emits by dequeuing messages from the queue. . Aggregating the log messages to a queue is the only way to handle this scenario. But there is a graceful alternative way to achieve this. Python’s logging module provides built in mechanisms to handle queue based logging with QueueHandler and QueueListener classes. . Similar to how FileHandler provides a way to do file based logging, QueueHandler helps to log to a queue object. This queue can be from the queue module or multiprocessing.Queue. . import logging from logging.handlers import QueueHandler from queue import SimpleQueue logging.warning(&quot;Test this warning in the console&quot;) q = SimpleQueue() queue_handler = QueueHandler(q) root_logger = logging.getLogger() # remove the StreamHandler instance that is set by default root_logger.handlers.pop() # add queue handler # this will stop the log messages from printing in the console root_logger.addHandler(queue_handler) logging.warning(&quot;Test this warning in the queue&quot;) print(q.get()) . This prints: . WARNING:root:Test this warning in the console &lt;LogRecord: root, 30, test.py, 14, &quot;Test this warning in the queue&quot;&gt; . Note the second printed line is only because of the print function. The second logging.warning function has consumed the message and put it in the q object. . QueueListener is used along with the QueueHandler to collect messages from the queue and push them to various other targets such as a file, UDP port, etc. . ... file_handler = FileHandler(&quot;out.log&quot;) listener = QueueListener(q, file_handler) listener.start() ... . The listener looks for messages in q and pushes them to file_handler which writes to out.log. Let’s use the same flow in a multi process setup. . Assume a master process that spawns 4 children and all log messages are to be written to out.log file. Any logger message from the children will be written to a multiprocessing.Queue. The children need to initialize a QueueHandler and attach it to their root logger as seen above to achieve this. The same queue is listened at the master process’s QueueListener. . Here’s how the master process’s logging is setup: . def setup_primary_logging(): log_queue = mp.Queue(-1) # Handlers for stream/file logging output_file_log_handler = logging.FileHandler(filename=&quot;out.log&quot;) formatter = logging.Formatter(&#39;%(asctime)s - %(levelname)s - %(message)s&#39;) output_file_log_handler.setFormatter(formatter) output_file_log_handler.setLevel(logging.INFO) # This listener listens to the `log_queue` and pushes the messages to the list of # handlers specified. listener = QueueListener(log_queue, output_file_log_handler, error_file_log_handler, respect_handler_level=True) listener.start() return log_queue . And the worker’s logging setup: . def setup_worker_logging(log_queue): queue_handler = QueueHandler(log_queue) queue_handler.setLevel(logging.INFO) root_logger = logging.getLogger() root_logger.addHandler(queue_handler) # Default logger level is WARNING, hence the change. Otherwise, any worker logs # are not going to get bubbled up to the parent&#39;s logger handlers from where the # actual logs are written to the output root_logger.setLevel(logging.INFO) . Note that log_queue object has to be sent from the master to all the children. So the master process looks something like this: . def worker_fn(log_queue): setup_worker_logging(log_queue) def master_fn(): log_queue = setup_primary_logging() mp.spawn(worker_fn, args=(log_queue,), n_procs=4) . Any messages from the worker will be logged to out.log file with the above setup. A complete working example can be seen in this gist. .",
            "url": "https://scarecrow1123.github.io/python/software/2019/09/26/multi-log.html",
            "relUrl": "/python/software/2019/09/26/multi-log.html",
            "date": " • Sep 26, 2019"
        }
        
    
  
    
        ,"post6": {
            "title": "OMS CS7638 Robotics - AI Techniques - Summer '19",
            "content": "CS 7638 Robotics - AI Techniques . Instructor(s): Jay Summet / Sebastian Thrun Course Page: Link . This happened to be my second OMS course following KBAI. Previously known as AI for Robotics(AI4R popularly), this isn’t supposed to be originally designed for a summer(short) term. This is typically a 16-week long course which got shrunk to a 11-week one and we were duly cautioned by the professor about this at the beginning of the term. . Course Videos . The course follows the content from Sebastian Thrun’s Udacity videos. The video lectures broadly covers the following: . Localization Histogram Filters | Kalman Filters | Particle Filters | . | Search / Path Planning | PID Control | SLAM (Simultaneous Localization and Planning) | . The video content are fairly simple to follow as Prof Thrun makes it sound easy in a lot of places. However, at times, the details were skimmed over and not a lot of background motivation were given for certain parts of the lecture material. Each module is followed by a problem set which comes with solution videos. Solving them and submitting them on canvas fetches 28% of the total grade of the entire class. These were not hard to solve and are not time consuming too. . Projects . The projects are the crux of this class as there are no tests/exams in this class. All projects are auto-graded and the solutions had to be adopted from the lecture content. For most of the projects, code from the problem sets served as the boilerplate and were highly helpful to set something up working quickly. The following projects were part of the Summer class: . Asteroids - Kalman Filter | Mars Glider - Particle Filter | Rocket PID - PID Control | Warehouse - Motion Planning and Search | Ice Rover - SLAM | . Asteroids and Mars Glider projects were being introduced for the first time as a part of CS7638 and we were the lab rats of some sorts. There was too much FOMO especially during the Mars Glider project weeks, as there were bugs in the testing system and also the test cases were being changed often as this was the first time they introduced the project. We also had to spend a lot of time in tuning parameters to make these systems work well. All of these projects deal with tackling noise in the measurements and motion of robots, obviously one had to get the parameters right to make the systems stable. People spent more time in tuning the params than they did for coding up the whole thing. So those who started late(like me in a couple of projects) suffered from lower scores. Starting early on the projects especially Mars Glider and Warehouse is advisable for anyone who do not want to miss on an A. . There were also constraints that were introduced in the projects which were not part of the lectures nor the problem sets. Examples being the Q-matrix in Kalman Filters, “fuzzing” in Mars Glider (which automatically made my code work). Typically none of us knew what they were about until the the teaching staff explained the entire thing and helped the class through. . Class and Grading . Prof Summet and the TAs did an excellent job in answering the student questions during the entire period of the class. Since there were a lot of curve balls thrown at us during the projects especially, their insights were very useful in coding up the projects and clarifying the doubts from the problem sets. The Slack group was also highly active with healthy discussions and suggestions. . Grading is absolute and the project scores are the only ones taken into account for the final grades apart from the problem set grades. Problem sets can be taken for granted as they are there only to check if one has finished watching the lectures. . Material . I personally did not follow Prof Thrun’s Probabilistic Robotics book which was the suggested text. This particular e-book Kalman and Bayesian Filters in Python is an excellent to resource for anyone to understand and get an intuition about these topics with straightforward implementation details. . Conclusion . Comparing to my previous KBAI class, there are no reports to be written and having code only projects are a huge plus. Optimizing for the auto-grader score games up the work being done and could turn to be fun, but only if you start early on for the projects! .",
            "url": "https://scarecrow1123.github.io/gatech-omscs/2019/07/31/rait.html",
            "relUrl": "/gatech-omscs/2019/07/31/rait.html",
            "date": " • Jul 31, 2019"
        }
        
    
  
    
        ,"post7": {
            "title": "Exploring Jsonnet",
            "content": "I found Jsonnet through AllenNLP. Hence a few words on that first. We use AllenNLP for writing our Deep Learning experiments in our team. It is primarily built for doing NLP research on top of PyTorch. However, the abstractions in the library are well designed and easily extensible that it can actually be used for building any fairly straightforward neural network experiments. Perhaps I would write a separate post on how to adopt AllenNLP for non-NLP experiments, but here is an example of how it has been used for Computer Vision. . Jsonnet . Jsonnet is a DSL for creating data templates and comes in handy to generate JSON based configuration data. It comes with a standard library std that includes features like list comprehension, string manipulation, etc. It is primarily meant for generating configuration files. std has a bunch of manifestation utilities that can be used to convert the template to generate targets in .ini, .yaml. For a robust templating language with more complex needs however, I’d suggest to use the awesome StringTemplate. . AllenNLP uses Jsonnet for writing experiment configurations. In other words, the dependencies for running an AllenNLP experiment are specified in a .jsonnet file and the objects are constructed using built in factories. Below is a section from a configuration that defines a simple feedforward MNIST classifier network: . Sample Configuration . mnist_feedforward.jsonnet . { // ..... &quot;model&quot;: { &quot;mnist_encoder&quot;: { &quot;num_layers&quot;: 2, &quot;activations&quot;: [&quot;relu&quot;, &quot;relu&quot;], &quot;input_dim&quot;: 784, &quot;hidden_dims&quot;: 512, &quot;dropout&quot;: 0.25 }, &quot;projection_layer&quot;: { &quot;num_layers&quot;: 1, &quot;activations&quot;: &quot;relu&quot;, &quot;input_dim&quot;: 512, &quot;hidden_dims&quot;: 64, &quot;dropout&quot;: 0.25 }, &quot;final_layer&quot;: { &quot;num_layers&quot;: 1, &quot;activations&quot;: &quot;linear&quot;, &quot;input_dim&quot;: 64, &quot;hidden_dims&quot;: 10 } } // ..... } . Variables . One minor quibble with the above config: when running multiple experiments, the most obvious thing one would do is to change those numbers in every layer. Say to increase the output dimension of the mnist_encoder, input_dim of projection_layer needs to be adjusted too. For a more complex architecture, there is a good possibility that this would lead to a chain of changes to be done manually. . Obvious thing to do now is to use variables. In the below example, notice how the same variable is used for configuring both the output and input sizes of mnist_encoder and projection_layer layers respectively. . mnist_feedforward.jsonnet . local INPUT_SIZE = 784; local INPUT_ENCODER_OUTPUT_SIZE = 512; local PROJECTION_SIZE = 64; local FINAL_OUTPUT_SIZE = 2; local DROPOUT = 0.25; { // ..... &quot;model&quot;: { &quot;mnist_encoder&quot;: { &quot;num_layers&quot;: 2, &quot;activations&quot;: [&quot;relu&quot;, &quot;relu&quot;], &quot;input_dim&quot;: INPUT_SIZE, &quot;hidden_dims&quot;: INPUT_ENCODER_OUTPUT_SIZE, &quot;dropout&quot;: DROPOUT }, &quot;projection_layer&quot;: { &quot;num_layers&quot;: 1, &quot;activations&quot;: &quot;relu&quot;, &quot;input_dim&quot;: INPUT_ENCODER_OUTPUT_SIZE, &quot;hidden_dims&quot;: PROJECTION_SIZE, &quot;dropout&quot;: 0.25 }, &quot;final_layer&quot;: { &quot;num_layers&quot;: 1, &quot;activations&quot;: &quot;linear&quot;, &quot;input_dim&quot;: PROJECTION_SIZE, &quot;hidden_dims&quot;: FINAL_OUTPUT_SIZE } } // ..... } . Objects . The next natural step of the experiment is to try different types of layers. In the above example mnist_encoder is a feedforward block. It could be a convolution based encoder as below: . mnist_conv.jsonnet . local ConvSpec = { &quot;num_layers&quot;: 2, &quot;input_dim&quot;: 784, &quot;kernels&quot;: [[3, 3], [3, 3]], &quot;stride&quot;: [1, 1], &quot;activations&quot;: [&quot;relu&quot;, &quot;relu&quot;], &quot;output_channels&quot;: [32, 64] }; { // .... &quot;model&quot;: { &quot;mnist_encoder&quot;: { &quot;type&quot;: &quot;conv2d&quot; &quot;num_layers&quot;: ConvSpec.num_layers, &quot;input_dim&quot;: ConvSpec.input_dim, &quot;output_channels&quot;: ConvSpec.output_channels, &quot;kernels&quot;: ConvSpec.kernels, &quot;stride&quot;: ConvSpec.stride, &quot;activations&quot;: ConvSpec.activations }, // .... }, // ..... } . Here ConvSpec is a jsonnet object with a bunch of member attributes that defines how the convolution block is used in the classifier. Jsonnet also supports inheritance of objects as shown here. . Functions . The subsequent layers such as projection_layer and final_layer are going to be present in the conv example too and the projection_layer needs an input size to be defined. This will be output size of the conv layer and hardcoding this number is going to cause the same set of problems that we saw above in the first example. Let’s define a simple function that computes the output sizes of each layer in a conv block. . local conv_output(dim, kernel, stride, padding, dilation) = std.floor(((dim + 2 * padding - dilation * (kernel - 1) -1) / stride) + 1); local compute_conv_output_sizes(conv_, input_size, axis, curr_idx=0, sizes=[]) = if curr_idx &gt;= conv_.num_layers then sizes else if std.length(sizes) == 0 then compute_conv_output_sizes(conv_, input_size, axis, curr_idx+1, [conv_output(input_size, conv_.kernels[curr_idx][axis], conv_.stride[curr_idx][axis], conv_.padding[curr_idx][axis], conv_.dilation[curr_idx][axis])]) else compute_conv_output_sizes(conv_, input_size, axis, curr_idx+1, sizes + [conv_output(sizes[std.length(sizes)-1], conv_.kernels[curr_idx][axis], conv_.stride[curr_idx][axis], conv_.padding[curr_idx][axis], conv_.dilation[curr_idx][axis])]); . compute_conv_output_sizes is a tail recursive function calculates the output size of each conv layer based on the defined kernel size, stride, padding and dilation. . Let’s incorporate this definition in the mnist_encoder example: . mnist_conv.jsonnet . local ConvSpec = { &quot;num_layers&quot;: 2, &quot;input_dim&quot;: 784, &quot;kernels&quot;: [[3, 3], [3, 3]], &quot;stride&quot;: [1, 1], &quot;activations&quot;: [&quot;relu&quot;, &quot;relu&quot;], &quot;output_channels&quot;: [32, 64], &quot;output_sizes&quot;: compute_conv_output_sizes(self, self.input_dim, 0), &quot;output_size&quot;: self.output_sizes[-1] }; { // .... &quot;model&quot;: { &quot;mnist_encoder&quot;: { &quot;type&quot;: &quot;conv2d&quot; &quot;num_layers&quot;: ConvSpec.num_layers, &quot;input_dim&quot;: ConvSpec.input_dim, &quot;output_channels&quot;: ConvSpec.output_channels, &quot;kernels&quot;: ConvSpec.kernels, &quot;stride&quot;: ConvSpec.stride, &quot;activations&quot;: ConvSpec.activations }, // .... }, // ..... } . Now a layer that follows mnist_encoder can make use of ConvSpec.output_size to configure its input sizes. . Imports . So we have defined two variants of encoders here for a classifier. Except for the encoder all the other parts of the model configuration and training configuration are going to be the same. Let’s put the base scaffolding that defines the classifier in a .libsonnet file. This will serve as an importable lib that two different experiments can use. . lib-mnist.libsonnet . { MNIST(encoder, projection_size, final_output_size, dropout, num_projection_layers): { // ..... &quot;model&quot;: { &quot;mnist_encoder&quot;: encoder, &quot;projection_layer&quot;: { &quot;num_layers&quot;: num_projection_layers, &quot;activations&quot;: &quot;relu&quot;, &quot;input_dim&quot;: if std.objectHas(encoder, &quot;type&quot;) then encoder.output_size else encoder.hidden_dims, &quot;hidden_dims&quot;: projection_size, &quot;dropout&quot;: dropout }, &quot;final_layer&quot;: { &quot;num_layers&quot;: 1, &quot;activations&quot;: &quot;linear&quot;, &quot;input_dim&quot;: projection_size, &quot;hidden_dims&quot;: final_output_size } } // ..... } } . Notice above how the MNIST classifier has been parameterized which can be used from different variants of the architecture. Now, let’s redefine the feedforward variant to do the import. . mnist_feedforward.jsonnet . local lib = import &quot;lib-mnist.libsonnet&quot;; local INPUT_SIZE = 784; local INPUT_ENCODER_OUTPUT_SIZE = 512; local PROJECTION_SIZE = 64; local FINAL_OUTPUT_SIZE = 2; local DROPOUT = 0.25; local NUM_PROJECTION_LAYERS = 1; local ENCODER = { &quot;spec&quot;: { &quot;num_layers&quot;: 2, &quot;activations&quot;: [&quot;relu&quot;, &quot;relu&quot;], &quot;input_dim&quot;: INPUT_SIZE, &quot;hidden_dims&quot;: INPUT_ENCODER_OUTPUT_SIZE, &quot;dropout&quot;: DROPOUT } }; lib.MNIST(ENCODER.spec, PROJECTION_SIZE, FINAL_OUTPUT_SIZE, DROPOUT, NUM_PROJECTION_LAYERS); . mnist_conv.jsonnet . local lib = import &quot;lib-mnist.libsonnet&quot;; local INPUT_SIZE = 784; local PROJECTION_SIZE = 64; local FINAL_OUTPUT_SIZE = 2; local DROPOUT = 0.25; local NUM_PROJECTION_LAYERS = 1; local ConvSpec = { &quot;num_layers&quot;: 2, &quot;input_dim&quot;: INPUT_SIZE, &quot;kernels&quot;: [[3, 3], [3, 3]], &quot;stride&quot;: [1, 1], &quot;activations&quot;: [&quot;relu&quot;, &quot;relu&quot;], &quot;output_channels&quot;: [32, 64], &quot;output_sizes&quot;: compute_conv_output_sizes(self, self.input_dim, 0), &quot;output_size&quot;: self.output_sizes[-1] }; local ConvEncoder = { &quot;spec&quot;: { &quot;type&quot;: &quot;conv2d&quot; &quot;num_layers&quot;: ConvSpec.num_layers, &quot;input_dim&quot;: ConvSpec.input_dim, &quot;output_channels&quot;: ConvSpec.output_channels, &quot;kernels&quot;: ConvSpec.kernels, &quot;stride&quot;: ConvSpec.stride, &quot;activations&quot;: ConvSpec.activations } }; lib.MNIST(ConvEncoder.spec, PROJECTION_SIZE, FINAL_OUTPUT_SIZE, DROPOUT, NUM_PROJECTION_LAYERS) . Now this kind of a setup would allow to easily bring in rapid prototyping and experimentation. Just replace the encoder with RNN or Self Attention based layers and pass it to lib.MNIST. . Jsonnet brings in a lot of flexibility to defining configurations as we have seen above. There are a lot more interesting things that could be done with respect to configuring neural net experiments with Jsonnet. Imagine writing a grid search procedure in Jsonnet that would generate all possible configurations for the different hyperparameter combinations. That wouldn’t be too difficult I guess. A lot of interesting example experiment configurations can be found in AllenNLP’s source here. .",
            "url": "https://scarecrow1123.github.io/deep-learning/software/2019/07/06/jsonnet.html",
            "relUrl": "/deep-learning/software/2019/07/06/jsonnet.html",
            "date": " • Jul 6, 2019"
        }
        
    
  
    
        ,"post8": {
            "title": "OMS CS7637 - Knowledge-Based AI (KBAI) - Spring '19",
            "content": "CS7637 Knowledge-Based AI - Cognitive Systems (KBAI) - Spring ‘19 . Instructor: Prof David Joyner Course Page: Link . I applied for OMSCS in 2018 and joined the Spring ‘19 batch. I intend to do Machine Learning(ML) Specialization. KBAI is in no way related to anything that is machine learning and it does not fall under the core/elective courses that is required for the specialization. To satisfy the specialization requirements, I will be taking the following seven courses that are available throughout this program: . CS 6515 GA | CS7641 ML | CS6476 CV | CS7642 RL | CS7646 ML4T | CSE6242 DVA | CSE6250 BD4H | . Apart from the above courses few other courses that may be relevant for someone who is interested in ML/AI could be CS6601 Artifical Intelligence, CS7638 AI for Robotics. KBAI can be considered as a distant precursor to a typical AI course, but definitely not a prerequisite if one is planning to take any of the above courses. . About KBAI . KBAI is a gentle introduction to concepts and problems that are involved when designing an AI algorithm. The syllabus surveys a wide variety of traditional AI paradigms and concepts that closely follows the Patrick Winston “Artificial Intelligence” book. The course content also includes some of the work done personally by the instructors Ashok Goel, David Joyner and people from their lab. . Course work . The goal of this class is to develop an AI agent to solve a variant of an IQ test called Raven’s Progressive Matrices (RPM). The class is divided into three 5-week periods. Each of these include submitting a 10-page assignment, project work and an exam. The project work includes writing code to solve a given set of RPM problems and writing a detailed report. So at the end of the course everyone would have submitted 3 sets of assignments, projects and exams. . RPM Project . There are 4 sets of RPM problems given with increasing difficulty(see attached image for a sample problem). These problems could be solved in two different ways. For the first two projects, we would be given a verbal description of these patterns which can be used as input to solve the problem. However for the final project, only images can be directly used as inputs. Hence it would make sense to start the first project itself by using image inputs rather than starting with verbal and changing the agent implementation at a later stage to use visual inputs. These two papers[1], [2] helped a lot to implement the solution successfully. . Getting started early on the projects is also helpful. The grading rubric for the project includes how spaced out the submissions are for a project. . About the class . The class was very well organized. I should say that Prof Joyner’s way of organizing a class should be a blueprint for designing and conducting classes online. The TAs were also helpful throughout in Piazza and the forums were literally buzzing throughout(perhaps because posting something would fetch a student some participation grades too ;)). But overall the class was very engaging and set the tone for me on how OMS is going to be. The class also used Peerfeedback to receive and give peer feedback about project and assignment submissions. Even though the feedbacks I received were mostly generic and sometimes rhetorical, getting to read others’ submissions was definitely helpful in a lot of ways to me personally. . One quibble I had with the content was that the things I(or most of the class) implemented for the projects were for a large part looked disconnected from the lecture content. Even though on hindsight a bunch of things from an implementation could be mapped to a few concepts in the lectures, there was nothing that enforced on using them. Also, the lecture content looked repetitive and hand-wavy a bit at times. There are not many computational techniques/models that one could learn from this class. . There are also a couple of optional participation projects completing which would fetch some participation points. Both of them had an NLP problem to solve. These projects are testers for what may be given as full time projects in one of the later versions of KBAI. Another interesting fact about KBAI is from one of the discussions that happened in the forums. It looks like in the Fall version of KBAI, the TAs wouldn’t know if a submission is from the online class or the onsite class that happens at GT. And I heard that there is no much performance difference between the two classes. . Conclusion . I opted this class because this was my first semester into the program. A lot of reviews in OMSCentral also suggested this class for someone who is starting with OMS. I also intended to get an introduction to the traditional AI topics and this class did satisfy a part of my expectations. However, if you are expecting to learn computational techniques, this class is weak on that front. This is an easy class for a graduate level program and one can expect to work around 10-15 hours a week during submission times. .",
            "url": "https://scarecrow1123.github.io/gatech-omscs/2019/05/05/kbai.html",
            "relUrl": "/gatech-omscs/2019/05/05/kbai.html",
            "date": " • May 5, 2019"
        }
        
    
  
    
        ,"post9": {
            "title": "About Georgia Tech OMSCS",
            "content": "Online Master of Science Computer Science (OMSCS) . Georgia Tech(GT) introduced their Online Master’s program way back in 2014 IIRC. I remember reading one of the early Hacker News(HN) threads about the program when it was introduced. MOOCs were at rise during that period of time. But doing an entire Master’s program online was definitely new to me then and realized how cool the offering is when I discussed about it with my uncle who actually sent me the HN thread. On hindsight, now I understand how far ahead GT was in offering this course. For a perspective, lots of other universities such as Arizona, UIUC, etc. have started similar courses only very recently, while few thousands have already graduated from OMSCS in the last few years. . Back to 2018 . I decided to enrol myself into the program after realizing the importance of going through a high standard curriculum of a Master’s program. I was trying to learn the basics of Deep Learning myself from a bunch of great graduate level resources. My routine was to mostly watch lectures, read related material, take notes and apply them at work. Even though this exposed me to a lot of graduate level content, I still wasn’t committed much to the material because I wasn’t going through a formal curriculum that would include assignments, grades and stuff. . Choosing OMSCS . I started looking into the online graduate level programs. The primary factors that I looked into these programs were: . Credibility | Quality of course content | Affordability | Community | . Credibility &amp; Course content . GT has been a pioneer with more than 2000 alumni from OMS and I’ve heard good things about the program through my friends who are already enrolled to the program. So I had no second thoughts on the credibility of this program. People have even got into some top PhD programs after OMSCS. . The course listing is actually not bad. I agree that there may not be courses like Deep Learning which I was primarily interested on. However there’s a lot to learn from the current offering. The Machine Learning related courses do look solid. All of the courses are produced exclusively for the online program and many courses are siblings of the onsite offering but just optimized for an online experience. . Affordability . OMSCS scores big when it comes to affordability. The tuition fee for a single 3 credit course is ~ $500 and term fee is ~ $300. In comparison, the UIUC program may cost up to 3 times that of OMSCS. . Community . Doing a remote degree does have rough edges early on as a fresher into the program. Things can go wrong any time with respect to the classroom experience as well as administrative stuff such as registration, payment, etc. Troubleshooting is not easy even with the help of thorough documentation. OMSCS has got multiple friendly online communities at Reddit, Slack, Facebook, MeWe. The now defunct G+ community was the best of all though. These are all well moderated student run communities and people are extremely friendly and help you throughout. There is also OMSCentral volunteer project where students write their reviews and comments about the courses they’ve taken. This is super helpful in getting to validating and choosing courses that may suit you personally. . Applying . I applied in August 2018 for Spring ‘19 enrollment. The process involved submitting a TOEFL score along with a SoP, resume, background essay and 3 LoRs. OMSCS has a rolling admission process. It means that one can submit an application round the year. But there are cutoff dates for respective semesters. One can get enrolled into either Spring/Fall of a year. The academic calendar of OMSCS is the same as that of the onsite courses. I received the institute decision by October first week. I’ve heard the acceptance rate is pretty high at more than 60%, but I’m not sure. The reddit community actively maintains an admission thread which may shed more information on why one may get rejected if that is the case. . Getting into the program . Students will receive a detailed orientation document before the course registration starts. Things may get very anxious before the start of the registration. The G+ community was very helpful in clearing the doubts early and this set of Orientation videos by Prof Joyner turned out to be very helpful to get to know about how the courses would be run. . Program info . OMSCS requires one to complete 10 three credit courses to receive a degree. The actual degree offered does not differ from the one that is given for an onsite MS program. However, the transcripts would mention the campus as Online if that would matter. The schedule is a standard one following a three semester per year schedule which is the same as the onsite one. One can take upto 2 courses in Spring &amp; Fall and 1 in Summer. If one wishes to take an extra course, they need to get permission from the advising team. OMSCS also offers different specializations which may help to channelize one’s choice of courses throughout the program. Every specialization has core and elective courses requirement that needs to be satisfied. . Conclusion . I’m enjoying OMSCS so far and would definitely recommend to anyone who may be interested. I just finished my first semester(Spring ‘19) with CS7637 and will be writing about it in another post soon. People have written a lot about this program online and also given finer details about the courses. They have all been helpful for me in getting to know about this program. I intend to do the same thing by logging my OMS experience. .",
            "url": "https://scarecrow1123.github.io/gatech-omscs/2019/05/04/about-gatech.html",
            "relUrl": "/gatech-omscs/2019/05/04/about-gatech.html",
            "date": " • May 4, 2019"
        }
        
    
  
    
        ,"post10": {
            "title": "Python Control Flow: EAFP Vs LBYL",
            "content": "EAFP - Easier to Ask for Forgiveness than Permission LBYL - Look Before You Leap . These are two different ways to do control flow. LBYL style pertains to writing if/else blocks to make decisions. According to [1], in this not so standard Pythonic way of doing control flow, exceptional cases get the emphasis by the way the conditions are expressed. A common example as below: . if &quot;key&quot; in dict_: value += dict_[&quot;key&quot;] . As [1] suggests, in simpler words, the above piece of code conveys the special case in an emphasized way rather than showing us what is normal. Using try/except blocks(EAFP), we write what is normal and handle exceptions that may rise out of it. This becomes easier to convey the more natural cases as follows: . try: value += dict_[&quot;key&quot;] except KeyError: pass . [2] adds another important case where EAFP helps to avoid race conditions. In a multi-threaded environment, in the above if/else block, assume a thread has passed the if condition. Before the next statement gets executed in the current thread, another thread may inadvertently remove the key from dict_ which would cause an exception in the original thread. However, in the try/except case, this cannot happen. Another way to solve this problem is to use standard locking mechanisms. . [1] - Idiomatic Python: EAFP versus LBYL . [2] - Chapter 15. Context Managers and else Blocks, Fluent Python, Luciano Ramalho .",
            "url": "https://scarecrow1123.github.io/python/2019/04/22/python.html",
            "relUrl": "/python/2019/04/22/python.html",
            "date": " • Apr 22, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am Ananda Seelan and this is my blog (or rather just a list of interesting things that I would like to note down for myself or a few others who might find it interesting). .",
          "url": "https://scarecrow1123.github.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://scarecrow1123.github.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}